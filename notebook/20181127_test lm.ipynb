{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lmで次の単語を予測して、性質を理解する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import vocab, data\n",
    "import fastai\n",
    "import fire\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "import torch\n",
    "from fastai_contrib.utils import read_file, read_whitespace_file,\\\n",
    "    DataStump, validate, PAD, UNK, get_sentencepiece, PAD_TOKEN_ID\n",
    "\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'data/wiki/ja-2/'\n",
    "spm_dir = 'data/wiki/ja/'\n",
    "lang='ja'\n",
    "cuda_id=0\n",
    "qrnn=True\n",
    "subword=True\n",
    "max_vocab=16000\n",
    "bs=70\n",
    "bptt=70\n",
    "name='wt-2'\n",
    "num_epochs=1\n",
    "ds_pct=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "model_dir = 'models' # removed from params, as it is absolute models location in train_clas and here it is relative\n",
    "if not torch.cuda.is_available():\n",
    "    print('CUDA not available. Setting device=-1.')\n",
    "    cuda_id = -1\n",
    "torch.cuda.set_device(cuda_id)\n",
    "\n",
    "dir_path = Path(dir_path)\n",
    "assert dir_path.exists()\n",
    "model_dir = Path(model_dir)\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "if spm_dir:\n",
    "    spm_dir=Path(spm_dir)\n",
    "else:\n",
    "    spm_dir = dir_path\n",
    "assert spm_dir.exists()\n",
    "\n",
    "print('Batch size:', bs)\n",
    "print('Max vocab:', max_vocab)\n",
    "model_name = 'qrnn' if qrnn else 'lstm'\n",
    "if qrnn:\n",
    "    print('Using QRNNs...')\n",
    "\n",
    "trn_path = dir_path / f'{lang}.wiki.train.tokens'\n",
    "val_path = dir_path / f'{lang}.wiki.valid.tokens'\n",
    "tst_path = dir_path / f'{lang}.wiki.test.tokens'\n",
    "for path_ in [trn_path, val_path, tst_path]:\n",
    "    assert path_.exists(), f'Error: {path_} does not exist.'\n",
    "\n",
    "if subword:\n",
    "    # apply sentencepiece tokenization\n",
    "    trn_path = dir_path / f'{lang}.wiki.train.tokens'\n",
    "    val_path = dir_path / f'{lang}.wiki.valid.tokens'\n",
    "\n",
    "    read_file(trn_path, 'train')\n",
    "    read_file(val_path, 'valid')\n",
    "    \n",
    "    # assume sentencepiece training is done after merge of wiki\n",
    "    # here we're just loading the trained spm model\n",
    "    sp = get_sentencepiece(spm_dir, None, 'wt-all', vocab_size=max_vocab)\n",
    "    \n",
    "    data_lm = TextLMDataBunch.from_csv(dir_path, 'train.csv',\n",
    "                                        **sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = sp['vocab'].stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = sp['vocab'].itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_sz, nh, nl = 400, 1550, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(\n",
    "        data_lm, bptt=bptt, emb_sz=emb_sz, nh=nh, nl=nl, qrnn=qrnn,\n",
    "        pad_token=PAD_TOKEN_ID,\n",
    "        path = 'data/wiki/ja-100/',    \n",
    "        model_dir= 'models',\n",
    "        pretrained_fnames=['qrnn_wt-100', 'itos_wt-100'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageLearner(data=TextLMDataBunch;\n",
       "Train: LabelList\n",
       "y: LMLabel (69293 items)\n",
       "[Category 0, Category 0, Category 0, Category 0, Category 0]...\n",
       "Path: data/wiki/ja-2\n",
       "x: LMTextList (69293 items)\n",
       "[Text ▁ x x bo s ▁ x x f l d ▁ 1, Text ▁ x x bo s ▁ x x f l d ▁ 1 ▁15 48 年 10 月 、 ジャン ヌ ・ ダル ブレ と 第 2 代 ヴァン ドーム 公 アント ワー ヌ が 結婚 した 。 カトリック の 優勢 な ヴァン ドーム に 、 短期間 ユ グ ノー が 滞在 した 。 15 62 年 、 ユ グ ノー が サン = ジョ ル ジュ 教会 を 汚 し 略奪 した 。 17 93 年には 、 城 の 心臓 部 にある ブル ボン = ヴァン ドーム 家の 真 の ネ クロ ポリス が 略奪 に遭い 、 現在は 廃 <unk> となっている 。 アンリ 4 世は 城 の 包囲 へ 向かい 、 158 9 年に ヴァン ドーム は カトリック 同盟 軍に 降伏 した 。, Text ▁ x x bo s ▁ x x f l d ▁ 1, Text ▁ x x bo s ▁ x x f l d ▁ 1 ▁16 23 年 、 ヴァン ドーム 公 セ ザール ・ ド ・ ブル ボン は オラ トリオ 会 の神 学校 を つく った 。 これが 現在の リ セー ・ ロン サール である 。, Text ▁ x x bo s ▁ x x f l d ▁ 1]...\n",
       "Path: data/wiki/ja-2;\n",
       "Valid: LabelList\n",
       "y: LMLabel (17324 items)\n",
       "[Category 0, Category 0, Category 0, Category 0, Category 0]...\n",
       "Path: data/wiki/ja-2\n",
       "x: LMTextList (17324 items)\n",
       "[Text ▁ x x bo s ▁ x x f l d ▁ 1, Text ▁ x x bo s ▁ x x f l d ▁ 1 ▁『 お ひ さま 』 は 、2011 年 4 月 4 日から 同年 10 月 1 日まで 、 <unk> <unk> で放送された 『 連続 テレビ 小説 』 第 84 シリーズの 作品 。 『 連続 テレビ 小説 』 が 『 娘 と 私 』 放送開始 から 数え て 50 周年 となる の を 記念 する 作品として 位置 付けられた 。, Text ▁ x x bo s ▁ x x f l d ▁ 1, Text ▁ x x bo s ▁ x x f l d ▁ 1 ▁『 連続 テレビ 小説 』 では 珍しく 、「 私は 陽 子 。 太陽 の “ 陽 子 ” です !」 という 番組 キャッチ コピー が 設定された ( 番組 宣伝 で 繰り返し 用いられた ) 。, Text ▁ x x bo s ▁ x x f l d ▁ 1]...\n",
       "Path: data/wiki/ja-2;\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): RNNCore(\n",
       "    (encoder): Embedding(16000, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(16000, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): QRNNLayer(\n",
       "        (linear): WeightDropout(\n",
       "          (module): Linear(in_features=800, out_features=4650, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): QRNNLayer(\n",
       "        (linear): WeightDropout(\n",
       "          (module): Linear(in_features=1550, out_features=4650, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): QRNNLayer(\n",
       "        (linear): WeightDropout(\n",
       "          (module): Linear(in_features=1550, out_features=1200, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=16000, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=<function cross_entropy at 0x7f24045fba60>, metrics=[<function accuracy at 0x7f23be454378>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data/wiki/ja-100'), model_dir='models', callback_fns=[<class 'fastai.basic_train.Recorder'>], callbacks=[RNNTrainer(learn=LanguageLearner(data=TextLMDataBunch;\n",
       "Train: LabelList\n",
       "y: LMLabel (69293 items)\n",
       "[Category 0, Category 0, Category 0, Category 0, Category 0]...\n",
       "Path: data/wiki/ja-2\n",
       "x: LMTextList (69293 items)\n",
       "[Text ▁ x x bo s ▁ x x f l d ▁ 1, Text ▁ x x bo s ▁ x x f l d ▁ 1 ▁15 48 年 10 月 、 ジャン ヌ ・ ダル ブレ と 第 2 代 ヴァン ドーム 公 アント ワー ヌ が 結婚 した 。 カトリック の 優勢 な ヴァン ドーム に 、 短期間 ユ グ ノー が 滞在 した 。 15 62 年 、 ユ グ ノー が サン = ジョ ル ジュ 教会 を 汚 し 略奪 した 。 17 93 年には 、 城 の 心臓 部 にある ブル ボン = ヴァン ドーム 家の 真 の ネ クロ ポリス が 略奪 に遭い 、 現在は 廃 <unk> となっている 。 アンリ 4 世は 城 の 包囲 へ 向かい 、 158 9 年に ヴァン ドーム は カトリック 同盟 軍に 降伏 した 。, Text ▁ x x bo s ▁ x x f l d ▁ 1, Text ▁ x x bo s ▁ x x f l d ▁ 1 ▁16 23 年 、 ヴァン ドーム 公 セ ザール ・ ド ・ ブル ボン は オラ トリオ 会 の神 学校 を つく った 。 これが 現在の リ セー ・ ロン サール である 。, Text ▁ x x bo s ▁ x x f l d ▁ 1]...\n",
       "Path: data/wiki/ja-2;\n",
       "Valid: LabelList\n",
       "y: LMLabel (17324 items)\n",
       "[Category 0, Category 0, Category 0, Category 0, Category 0]...\n",
       "Path: data/wiki/ja-2\n",
       "x: LMTextList (17324 items)\n",
       "[Text ▁ x x bo s ▁ x x f l d ▁ 1, Text ▁ x x bo s ▁ x x f l d ▁ 1 ▁『 お ひ さま 』 は 、2011 年 4 月 4 日から 同年 10 月 1 日まで 、 <unk> <unk> で放送された 『 連続 テレビ 小説 』 第 84 シリーズの 作品 。 『 連続 テレビ 小説 』 が 『 娘 と 私 』 放送開始 から 数え て 50 周年 となる の を 記念 する 作品として 位置 付けられた 。, Text ▁ x x bo s ▁ x x f l d ▁ 1, Text ▁ x x bo s ▁ x x f l d ▁ 1 ▁『 連続 テレビ 小説 』 では 珍しく 、「 私は 陽 子 。 太陽 の “ 陽 子 ” です !」 という 番組 キャッチ コピー が 設定された ( 番組 宣伝 で 繰り返し 用いられた ) 。, Text ▁ x x bo s ▁ x x f l d ▁ 1]...\n",
       "Path: data/wiki/ja-2;\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): RNNCore(\n",
       "    (encoder): Embedding(16000, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(16000, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): QRNNLayer(\n",
       "        (linear): WeightDropout(\n",
       "          (module): Linear(in_features=800, out_features=4650, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): QRNNLayer(\n",
       "        (linear): WeightDropout(\n",
       "          (module): Linear(in_features=1550, out_features=4650, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): QRNNLayer(\n",
       "        (linear): WeightDropout(\n",
       "          (module): Linear(in_features=1550, out_features=1200, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=16000, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=<function cross_entropy at 0x7f24045fba60>, metrics=[<function accuracy at 0x7f23be454378>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data/wiki/ja-100'), model_dir='models', callback_fns=[<class 'fastai.basic_train.Recorder'>], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): QRNNLayer(\n",
       "    (linear): WeightDropout(\n",
       "      (module): Linear(in_features=800, out_features=4650, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): QRNNLayer(\n",
       "    (linear): WeightDropout(\n",
       "      (module): Linear(in_features=1550, out_features=4650, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): QRNNLayer(\n",
       "    (linear): WeightDropout(\n",
       "      (module): Linear(in_features=1550, out_features=1200, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): Embedding(16000, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(16000, 400, padding_idx=1)\n",
       "  )\n",
       "  (2): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=16000, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")]), bptt=70, alpha=2.0, beta=1.0, adjust=False)], layer_groups=[Sequential(\n",
       "  (0): QRNNLayer(\n",
       "    (linear): WeightDropout(\n",
       "      (module): Linear(in_features=800, out_features=4650, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): QRNNLayer(\n",
       "    (linear): WeightDropout(\n",
       "      (module): Linear(in_features=1550, out_features=4650, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): QRNNLayer(\n",
       "    (linear): WeightDropout(\n",
       "      (module): Linear(in_features=1550, out_features=1200, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): Embedding(16000, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(16000, 400, padding_idx=1)\n",
       "  )\n",
       "  (2): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=16000, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '今日', 'は']\n"
     ]
    }
   ],
   "source": [
    "s = '今日は'\n",
    "s_toks = sp['tokenizer'].process_all([s])\n",
    "s_toks = s_toks[0]\n",
    "print(s_toks)\n",
    "s_nums = [stoi.get(i, stoi[UNK]) for i in s_toks]\n",
    "s_var = Variable(torch.from_numpy(np.array(s_nums)))[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   5, 2857,    6])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_var[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the default tensor type at the top\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor if torch.cuda.is_available() \n",
    "                                                     else torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_model(m, s, l=50):\n",
    "    s_toks = sp['tokenizer'].process_all([s])\n",
    "    s_toks = s_toks[0]\n",
    "    print(s_toks)\n",
    "    s_nums = [stoi.get(i, stoi[UNK]) for i in s_toks]\n",
    "#     s_var = V(np.array(s_nums))[None]\n",
    "    s_var = Variable(torch.cuda.LongTensor(np.array(s_nums)))[None]\n",
    "    \n",
    "    m[0].bs=1\n",
    "    m.eval()\n",
    "    m.reset()\n",
    "\n",
    "    res, *_ = m(s_var)\n",
    "    print('...', end='')\n",
    "\n",
    "    for i in range(l):\n",
    "        r = torch.multinomial(res[-1].exp(), 2)\n",
    "        #r = torch.topk(res[-1].exp(), 2)[1]\n",
    "#         print(r)\n",
    "        if r.data[0] == 0:\n",
    "            r = r[1]\n",
    "        else:\n",
    "            r = r[0]\n",
    "#         print(to_np(r))\n",
    "        word = itos[to_np(r)]\n",
    "#         print(r.unsqueeze(0))\n",
    "        res, *_ = m(r.unsqueeze(0).unsqueeze(0))\n",
    "        print(word, end=' ')\n",
    "    m[0].bs=bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')  # or 'cpu'\n",
    "m = m.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '料金', 'が高い']\n",
      "...という 存在 国語 は もはや 阿 常 と 美 験 と は 思 われ ない 、 上 意 ついた 一 性を 愛 して 仲間 の 想い を ブール ジュ で 発表 するとともに ～ 勇 気 の あら いい 形で 働 いている 事例 の み だ し や 広い 米 項目 との関連 を 指摘 する 。 ▁ x x f l d ▁ 1 ▁ x x f l d ▁ 1 ▁また 、 全国大会 を終えた 後 、2008 年春 から の 一般 投票 から の 事務局 長 と 結果 が発表され 、 j ・ la ・ エ レン と おらず タイム ロス 。 編集 部 の一人 が 56 を るため に 、 さ す が に 31 世 -16 込んだ 。 ▁ x x up ▁ 営業 時間 は 8 月 23 日から 25 日の 午後 3 時に 始まり かけ 5 分 継続 して 開始された が ic b を 通り 、 その 延長 にあたった 。 ▁ x x f l d ▁ 1 ▁ x x f l d ▁ 1 ▁s ら 6 g b エンジン ▁ x x f l d ▁ 1 ▁ 近年では 現在でも 各種 の 大会 の名称 、 特別 編 制 の 特集 のほか 、 幾 十 "
     ]
    }
   ],
   "source": [
    "sample_model(m,'料金が高い',l=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 文法的に正しい単語は並んでいるようだ\n",
    "- しかし、文頭のxxfld_1タグや小文字化のxxupタグが邪魔。sentencepieceの学習時と言語モデルの学習時にrulesとspecial casesを揃えるべき\n",
    "    - 現状はspm学習時はrules, specialなし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_ulmfitscratch)",
   "language": "python",
   "name": "conda_ulmfitscratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
